#!/usr/bin/env python3
"""
GPUç¯å¢ƒæ£€æŸ¥è„šæœ¬
éªŒè¯æ·±åº¦å­¦ä¹ è®­ç»ƒæ‰€éœ€çš„GPUç¯å¢ƒæ˜¯å¦æ­£ç¡®é…ç½®
"""

import sys
import subprocess

def check_nvidia_driver():
    """æ£€æŸ¥NVIDIAé©±åŠ¨"""
    print("=== æ£€æŸ¥NVIDIAé©±åŠ¨ ===")
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, check=True)
        print("âœ… NVIDIAé©±åŠ¨æ­£å¸¸")
        print(f"é©±åŠ¨ä¿¡æ¯:\n{result.stdout.split('Driver Version:')[1].split('CUDA Version:')[0].strip()}")
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ NVIDIAé©±åŠ¨æœªå®‰è£…æˆ–ä¸å¯ç”¨")
        return False

def check_cuda():
    """æ£€æŸ¥CUDAå®‰è£…"""
    print("\n=== æ£€æŸ¥CUDAå®‰è£… ===")
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, check=True)
        print("âœ… CUDAå·²å®‰è£…")
        version_line = [line for line in result.stdout.split('\n') if 'release' in line][0]
        print(f"CUDAç‰ˆæœ¬: {version_line.strip()}")
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ CUDAæœªå®‰è£…æˆ–nvccä¸åœ¨PATHä¸­")
        return False

def check_pytorch():
    """æ£€æŸ¥PyTorch GPUæ”¯æŒ"""
    print("\n=== æ£€æŸ¥PyTorch GPUæ”¯æŒ ===")
    try:
        import torch
        print("âœ… PyTorchå·²å®‰è£…")
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        if torch.cuda.is_available():
            print("âœ… PyTorch CUDAæ”¯æŒæ­£å¸¸")
            print(f"CUDAç‰ˆæœ¬ (PyTorch): {torch.version.cuda}")
            print(f"å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
                print(f"  - æ˜¾å­˜: {props.total_memory / 1024**3:.1f} GB")
                print(f"  - è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
            return True
        else:
            print("âŒ PyTorchæ— æ³•ä½¿ç”¨CUDA")
            return False
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False

def check_memory():
    """æ£€æŸ¥ç³»ç»Ÿå†…å­˜"""
    print("\n=== æ£€æŸ¥ç³»ç»Ÿå†…å­˜ ===")
    try:
        import psutil
        mem = psutil.virtual_memory()
        total_gb = mem.total / 1024**3
        available_gb = mem.available / 1024**3
        
        print(f"æ€»å†…å­˜: {total_gb:.1f} GB")
        print(f"å¯ç”¨å†…å­˜: {available_gb:.1f} GB")
        
        if total_gb >= 32:
            print("âœ… å†…å­˜å……è¶³ (â‰¥32GB)")
            return True
        elif total_gb >= 16:
            print("âš ï¸  å†…å­˜é€‚ä¸­ (16-32GB) - å»ºè®®å¢åŠ å†…å­˜")
            return True
        else:
            print("âŒ å†…å­˜ä¸è¶³ (<16GB) - å¯èƒ½æ— æ³•å¤„ç†å¤§å‹æ•°æ®é›†")
            return False
    except ImportError:
        print("âŒ psutilæœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥å†…å­˜")
        return False

def check_dependencies():
    """æ£€æŸ¥å…¶ä»–ä¾èµ–åº“"""
    print("\n=== æ£€æŸ¥ä¾èµ–åº“ ===")
    required_packages = [
        'numpy', 'pandas', 'sklearn', 'matplotlib', 'tqdm', 
        'akshare', 'h5py'
    ]
    
    missing = []
    for package in required_packages:
        try:
            if package == 'sklearn':
                __import__('sklearn')
            else:
                __import__(package)
            print(f"âœ… {package}")
        except ImportError:
            print(f"âŒ {package}")
            missing.append(package)
    
    if missing:
        print(f"\nç¼ºå°‘ä¾èµ–: {', '.join(missing)}")
        print("è¯·è¿è¡Œ: pip install " + ' '.join(missing))
        return False
    else:
        print("âœ… æ‰€æœ‰ä¾èµ–åº“å·²å®‰è£…")
        return True

def test_gpu_training():
    """æµ‹è¯•GPUè®­ç»ƒèƒ½åŠ›"""
    print("\n=== GPUè®­ç»ƒæµ‹è¯• ===")
    try:
        import torch
        import torch.nn as nn
        import time
        
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•")
            return False
        
        device = torch.device("cuda:0")
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ¨¡å‹
        model = nn.Sequential(
            nn.Linear(100, 200),
            nn.ReLU(),
            nn.Linear(200, 1)
        ).to(device)
        
        # æµ‹è¯•æ•°æ®
        batch_size = 1024
        x = torch.randn(batch_size, 100).to(device)
        y = torch.randn(batch_size, 1).to(device)
        
        optimizer = torch.optim.Adam(model.parameters())
        criterion = nn.MSELoss()
        
        # çƒ­èº«
        for _ in range(5):
            optimizer.zero_grad()
            output = model(x)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()
        
        # æµ‹è¯•æ€§èƒ½
        torch.cuda.synchronize()
        start_time = time.time()
        
        for _ in range(100):
            optimizer.zero_grad()
            output = model(x)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()
        
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 100 * 1000  # ms
        print(f"âœ… GPUè®­ç»ƒæµ‹è¯•é€šè¿‡")
        print(f"å¹³å‡è®­ç»ƒæ—¶é—´: {avg_time:.2f} ms/batch")
        
        # æµ‹è¯•æ··åˆç²¾åº¦
        try:
            from torch.cuda.amp import autocast, GradScaler
            scaler = GradScaler()
            
            with autocast():
                output = model(x)
                loss = criterion(output, y)
            
            print("âœ… æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒæ­£å¸¸")
            return True
        except ImportError:
            print("âš ï¸  æ··åˆç²¾åº¦è®­ç»ƒä¸æ”¯æŒ (éœ€è¦PyTorch >= 1.6)")
            return True
            
    except Exception as e:
        print(f"âŒ GPUè®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ·±åº¦å­¦ä¹ GPUç¯å¢ƒæ£€æŸ¥")
    print("=" * 50)
    
    checks = [
        check_nvidia_driver(),
        check_cuda(),
        check_pytorch(),
        check_memory(),
        check_dependencies(),
        test_gpu_training()
    ]
    
    passed = sum(checks)
    total = len(checks)
    
    print("\n" + "=" * 50)
    print(f"æ£€æŸ¥ç»“æœ: {passed}/{total} é¡¹é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ ç¯å¢ƒé…ç½®å®Œç¾ï¼å¯ä»¥å¼€å§‹GPUè®­ç»ƒ")
        return 0
    elif passed >= total - 1:
        print("âš ï¸  ç¯å¢ƒåŸºæœ¬å¯ç”¨ï¼Œä½†å»ºè®®è§£å†³å‰©ä½™é—®é¢˜")
        return 1
    else:
        print("âŒ ç¯å¢ƒé…ç½®æœ‰é‡å¤§é—®é¢˜ï¼Œè¯·ä¿®å¤åå†è¯•")
        return 2

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)